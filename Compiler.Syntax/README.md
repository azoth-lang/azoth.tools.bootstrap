# Azoth.Tools.Bootstrap.Compiler.Syntax

This project contains a *relatively* concrete syntax tree generated by the parser. It should not
contain any semantic analysis. The nodes do not contain parent references.

Currently, this tree and the parser make some semantic distinctions they should not. For example,
disallowing certain expressions to the left of an assignment and disallowing certain definitions
with traits, structs and classes.

Both tokens and syntax nodes have a separate interface and implementation layer. Other parts of the
compiler deal only with the interfaces. For both the tokens and syntax tree, transitioning to this
separation brought a lot of improvement. Even though it produces more code, the flexibility of
multiple inheritance hierarchies is very powerful and enables stronger typing.

The syntax tree is designed to be as concrete as possible. Meaning that each different kind of
syntax construct has a separate node type. For example, it used to be the case that functions,
methods, and associated functions were all the same node type. Now, each has a different node type.
This has improved type safety by ensuring nodes have exactly the properties that make sense for them
and that those properties have correct nullability. However, perhaps this has gone to far and
functions and associated functions should actually be the same node type since there doesn't seem to
be any difference between them other than their context.

## Naming Conventions

Each syntax interface ends in `Syntax` to distinguish them. The category of the node is also in the
name. While including the category is more verbose, not doing so leads to inconsistencies. There are
always some node types for which leaving out the category produces a confusing name. Categories are:

* Definition
* Statement
* Expression
  * OperatorExpression
  * LiteralExpression
  * ConversionExpression
  * InvocationExpression
* Parameter
* Type

## Traversing the Syntax Tree

Tree traversal is done either with a tree walker or by matching on node type. Tree walkers are
similar to the listener types of ANTLR. They provide an internal traversal. That is, the nodes are
walked and the methods of the walker are called as nodes are visited. The other, more common way of
traversing the tree is simply by matching on node type and recursing down the tree. When doing this,
having more generic/broad switches can help to insulate traversals from tree changes. That is, if a
traversal relies on a single large switch over all node types rather than on separate methods with
separate switches for different node categories, fewer changes are required when the tree types
change.

It took a while to arrive at these as the traversal approach. Methods on the nodes with overrides
were ruled out early since there may be many traversals and they are too hard to work on if they are
spread out among all the node classes. Instead, the visitor pattern was originally the preferred
method of traversing the AST. This was very cumbersome though. It also made it very difficult to
handle multiple node types as a single case. At one point, there was experimentation with using
reflection to safely implement visitors but that was confusing to refactoring tools and spread the
code out among many methods just like a visitor. Using switch statements to match on type was a near
ideal syntax, but without exhaustiveness checking, it was incredibly error prone. With the creation
and use of the `ExhaustiveMatching.Analyzer` package that provides the most flexible exhaustiveness
checking of any language, that has changed. Now switching on type is the preferred approach in many
cases.

## Design Goals

### Adaptability to Syntax Changes

As the language is still being designed, it is important that the lexer, parser and concrete syntax
tree be easy to modify. This is supported by keeping them as simple as possible. Code generation is
used around tokens so that tokens can be easily added and removed. Several points in the lexer use
generated lists of all tokens of a given type to guide the lexing process.

### Correct by Construction

Originally, tokens were not as strongly typed. They were using several struct types with an enum for
the token type. This performance optimization was modelled on the Roslyn C# compiler. However, it
was decided that having strongly typed tokens was more valuable and the transition was made to the
current set of classes. This also allowed for more sophisticated type relationships such as a base
class for keywords and a base class for operators. Ideally, missing tokens would be represented with
a special token that included the type and position of the missing token. However, the C# type
system didn't offer good options for representing that in a strongly typed way. It was decided to
use `null` to represent missing tokens and accept the limitations that might imply.
